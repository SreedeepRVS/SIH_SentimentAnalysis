{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import requests\n",
    "import codecs\n",
    "import json\n",
    "from itertools import product\n",
    "from inspect import getsourcefile\n",
    "from io import open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (empirically derived mean sentiment intensity rating increase for booster words)\n",
    "B_INCR = 0.293\n",
    "B_DECR = -0.293\n",
    "\n",
    "# (empirically derived mean sentiment intensity rating increase for using ALLCAPs to emphasize a word)\n",
    "C_INCR = 0.733\n",
    "N_SCALAR = -0.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGATE = \\\n",
    "    [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    "     \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    "     \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    "     \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    "     \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
    "     \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    "     \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    "     \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOSTER_DICT = \\\n",
    "    {\"absolutely\": B_INCR, \"amazingly\": B_INCR, \"awfully\": B_INCR, \n",
    "     \"completely\": B_INCR, \"considerable\": B_INCR, \"considerably\": B_INCR,\n",
    "     \"decidedly\": B_INCR, \"deeply\": B_INCR, \"effing\": B_INCR, \"enormous\": B_INCR, \"enormously\": B_INCR,\n",
    "     \"entirely\": B_INCR, \"especially\": B_INCR, \"exceptional\": B_INCR, \"exceptionally\": B_INCR, \n",
    "     \"extreme\": B_INCR, \"extremely\": B_INCR,\n",
    "     \"fabulously\": B_INCR, \"flipping\": B_INCR, \"flippin\": B_INCR, \"frackin\": B_INCR, \"fracking\": B_INCR,\n",
    "     \"fricking\": B_INCR, \"frickin\": B_INCR, \"frigging\": B_INCR, \"friggin\": B_INCR, \"fully\": B_INCR, \n",
    "     \"fuckin\": B_INCR, \"fucking\": B_INCR, \"fuggin\": B_INCR, \"fugging\": B_INCR,\n",
    "     \"greatly\": B_INCR, \"hella\": B_INCR, \"highly\": B_INCR, \"hugely\": B_INCR, \n",
    "     \"incredible\": B_INCR, \"incredibly\": B_INCR, \"intensely\": B_INCR, \n",
    "     \"major\": B_INCR, \"majorly\": B_INCR, \"more\": B_INCR, \"most\": B_INCR, \"particularly\": B_INCR,\n",
    "     \"purely\": B_INCR, \"quite\": B_INCR, \"really\": B_INCR, \"remarkably\": B_INCR,\n",
    "     \"so\": B_INCR, \"substantially\": B_INCR,\n",
    "     \"thoroughly\": B_INCR, \"total\": B_INCR, \"totally\": B_INCR, \"tremendous\": B_INCR, \"tremendously\": B_INCR,\n",
    "     \"uber\": B_INCR, \"unbelievably\": B_INCR, \"unusually\": B_INCR, \"utter\": B_INCR, \"utterly\": B_INCR,\n",
    "     \"very\": B_INCR,\n",
    "     \"almost\": B_DECR, \"barely\": B_DECR, \"hardly\": B_DECR, \"just enough\": B_DECR,\n",
    "     \"kind of\": B_DECR, \"kinda\": B_DECR, \"kindof\": B_DECR, \"kind-of\": B_DECR,\n",
    "     \"less\": B_DECR, \"little\": B_DECR, \"marginal\": B_DECR, \"marginally\": B_DECR,\n",
    "     \"occasional\": B_DECR, \"occasionally\": B_DECR, \"partly\": B_DECR,\n",
    "     \"scarce\": B_DECR, \"scarcely\": B_DECR, \"slight\": B_DECR, \"slightly\": B_DECR, \"somewhat\": B_DECR,\n",
    "     \"sort of\": B_DECR, \"sorta\": B_DECR, \"sortof\": B_DECR, \"sort-of\": B_DECR}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_LADEN_IDIOMS = {\"cut the mustard\": 2, \"hand to mouth\": -2,\n",
    "                          \"back handed\": -2, \"blow smoke\": -2, \"blowing smoke\": -2,\n",
    "                          \"upper hand\": 1, \"break a leg\": 2,\n",
    "                          \"cooking with gas\": 2, \"in the black\": 2, \"in the red\": -2,\n",
    "                          \"on the ball\": 2, \"under the weather\": -2}\n",
    "\n",
    "# check for special case idioms containing lexicon words\n",
    "SPECIAL_CASE_IDIOMS = {\"the shit\": 3, \"the bomb\": 3, \"bad ass\": 1.5, \"badass\": 1.5,\n",
    "                       \"yeah right\": -2, \"kiss of death\": -1.5, \"to die for\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negated(input_words, include_nt=True):\n",
    "    \"\"\"\n",
    "    Determine if input contains negation words\n",
    "    \"\"\"\n",
    "    input_words = [str(w).lower() for w in input_words]\n",
    "    neg_words = []\n",
    "    neg_words.extend(NEGATE)\n",
    "    for word in neg_words:\n",
    "        if word in input_words:\n",
    "            return True\n",
    "    if include_nt:\n",
    "        for word in input_words:\n",
    "            if \"n't\" in word:\n",
    "                return True\n",
    "    '''if \"least\" in input_words:\n",
    "        i = input_words.index(\"least\")\n",
    "        if i > 0 and input_words[i - 1] != \"at\":\n",
    "            return True'''\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(score, alpha=15):\n",
    "    \"\"\"\n",
    "    Normalize the score to be between -1 and 1 using an alpha that\n",
    "    approximates the max expected value\n",
    "    \"\"\"\n",
    "    norm_score = score / math.sqrt((score * score) + alpha)\n",
    "    if norm_score < -1.0:\n",
    "        return -1.0\n",
    "    elif norm_score > 1.0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return norm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allcap_differential(words):\n",
    "    \"\"\"\n",
    "    Check whether just some words in the input are ALL CAPS\n",
    "    :param list words: The words to inspect\n",
    "    :returns: `True` if some but not all items in `words` are ALL CAPS\n",
    "    \"\"\"\n",
    "    is_different = False\n",
    "    allcap_words = 0\n",
    "    for word in words:\n",
    "        if word.isupper():\n",
    "            allcap_words += 1\n",
    "    cap_differential = len(words) - allcap_words\n",
    "    if 0 < cap_differential < len(words):\n",
    "        is_different = True\n",
    "    return is_different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalar_inc_dec(word, valence, is_cap_diff):\n",
    "    \"\"\"\n",
    "    Check if the preceding words increase, decrease, or negate/nullify the\n",
    "    valence\n",
    "    \"\"\"\n",
    "    scalar = 0.0\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in BOOSTER_DICT:\n",
    "        scalar = BOOSTER_DICT[word_lower]\n",
    "        if valence < 0:\n",
    "            scalar *= -1\n",
    "        # check if booster/dampener word is in ALLCAPS (while others aren't)\n",
    "        if word.isupper() and is_cap_diff:\n",
    "            if valence > 0:\n",
    "                scalar += C_INCR\n",
    "            else:\n",
    "                scalar -= C_INCR\n",
    "    return scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentiText(object):\n",
    "    \"\"\"\n",
    "    Identify sentiment-relevant string-level properties of input text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text).encode('utf-8')\n",
    "        self.text = text\n",
    "        self.words_and_emoticons = self._words_and_emoticons()\n",
    "        # doesn't separate words from\\\n",
    "        # adjacent punctuation (keeps emoticons & contractions)\n",
    "        self.is_cap_diff = allcap_differential(self.words_and_emoticons)\n",
    "\n",
    "    @staticmethod\n",
    "    def _strip_punc_if_word(token):\n",
    "        \"\"\"\n",
    "        Removes all trailing and leading punctuation\n",
    "        If the resulting string has two or fewer characters,\n",
    "        then it was likely an emoticon, so return original string\n",
    "        (ie \":)\" stripped would be \"\", so just return \":)\"\n",
    "        \"\"\"\n",
    "        stripped = token.strip(string.punctuation)\n",
    "        if len(stripped) <= 2:\n",
    "            return token\n",
    "        return stripped\n",
    "\n",
    "    def _words_and_emoticons(self):\n",
    "        \"\"\"\n",
    "        Removes leading and trailing puncutation\n",
    "        Leaves contractions and most emoticons\n",
    "            Does not preserve punc-plus-letter emoticons (e.g. :D)\n",
    "        \"\"\"\n",
    "        wes = self.text.split()\n",
    "        stripped = list(map(self._strip_punc_if_word, wes))\n",
    "        return stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentIntensityAnalyzer(object):\n",
    "    \"\"\"\n",
    "    Give a sentiment intensity score to sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lexicon_file=\"vader_lexicon.txt\", emoji_lexicon=\"emoji_utf8_lexicon.txt\"):\n",
    "        _this_module_file_path_ = os.path.abspath(getsourcefile(lambda: 0))\n",
    "        lexicon_full_filepath = os.path.join(os.path.dirname(_this_module_file_path_), lexicon_file)\n",
    "        with codecs.open(lexicon_full_filepath, encoding='utf-8') as f:\n",
    "            self.lexicon_full_filepath = f.read()\n",
    "        self.lexicon = self.make_lex_dict()\n",
    "\n",
    "        emoji_full_filepath = os.path.join(os.path.dirname(_this_module_file_path_), emoji_lexicon)\n",
    "        with codecs.open(emoji_full_filepath, encoding='utf-8') as f:\n",
    "            self.emoji_full_filepath = f.read()\n",
    "        self.emojis = self.make_emoji_dict()\n",
    "\n",
    "    def make_lex_dict(self):\n",
    "        \"\"\"\n",
    "        Convert lexicon file to a dictionary\n",
    "        \"\"\"\n",
    "        lex_dict = {}\n",
    "        for line in self.lexicon_full_filepath.rstrip('\\n').split('\\n'):\n",
    "            if not line:\n",
    "                continue\n",
    "            (word, measure) = line.strip().split('\\t')[0:2]\n",
    "            lex_dict[word] = float(measure)\n",
    "        return lex_dict\n",
    "\n",
    "    def make_emoji_dict(self):\n",
    "        \"\"\"\n",
    "        Convert emoji lexicon file to a dictionary\n",
    "        \"\"\"\n",
    "        emoji_dict = {}\n",
    "        for line in self.emoji_full_filepath.rstrip('\\n').split('\\n'):\n",
    "            (emoji, description) = line.strip().split('\\t')[0:2]\n",
    "            emoji_dict[emoji] = description\n",
    "        return emoji_dict\n",
    "\n",
    "    def polarity_scores(self, text):\n",
    "        \"\"\"\n",
    "        Return a float for sentiment strength based on the input text.\n",
    "        Positive values are positive valence, negative value are negative\n",
    "        valence.\n",
    "        \"\"\"\n",
    "        # convert emojis to their textual descriptions\n",
    "        text_no_emoji = \"\"\n",
    "        prev_space = True\n",
    "        for chr in text:\n",
    "            if chr in self.emojis:\n",
    "                # get the textual description\n",
    "                description = self.emojis[chr]\n",
    "                if not prev_space:\n",
    "                    text_no_emoji += ' '\n",
    "                text_no_emoji += description\n",
    "                prev_space = False\n",
    "            else:\n",
    "                text_no_emoji += chr\n",
    "                prev_space = chr == ' '\n",
    "        text = text_no_emoji.strip()\n",
    "\n",
    "        sentitext = SentiText(text)\n",
    "\n",
    "        sentiments = []\n",
    "        words_and_emoticons = sentitext.words_and_emoticons\n",
    "        for i, item in enumerate(words_and_emoticons):\n",
    "            valence = 0\n",
    "            # check for vader_lexicon words that may be used as modifiers or negations\n",
    "            if item.lower() in BOOSTER_DICT:\n",
    "                sentiments.append(valence)\n",
    "                continue\n",
    "            if (i < len(words_and_emoticons) - 1 and item.lower() == \"kind\" and\n",
    "                    words_and_emoticons[i + 1].lower() == \"of\"):\n",
    "                sentiments.append(valence)\n",
    "                continue\n",
    "\n",
    "            sentiments = self.sentiment_valence(valence, sentitext, item, i, sentiments)\n",
    "\n",
    "        sentiments = self._but_check(words_and_emoticons, sentiments)\n",
    "\n",
    "        valence_dict = self.score_valence(sentiments, text)\n",
    "\n",
    "        return valence_dict\n",
    "\n",
    "    def sentiment_valence(self, valence, sentitext, item, i, sentiments):\n",
    "        is_cap_diff = sentitext.is_cap_diff\n",
    "        words_and_emoticons = sentitext.words_and_emoticons\n",
    "        item_lowercase = item.lower()\n",
    "        if item_lowercase in self.lexicon:\n",
    "            # get the sentiment valence \n",
    "            valence = self.lexicon[item_lowercase]\n",
    "                \n",
    "            # check for \"no\" as negation for an adjacent lexicon item vs \"no\" as its own stand-alone lexicon item\n",
    "            if item_lowercase == \"no\" and words_and_emoticons[i + 1].lower() in self.lexicon:\n",
    "                # don't use valence of \"no\" as a lexicon item. Instead set it's valence to 0.0 and negate the next item\n",
    "                valence = 0.0\n",
    "            if (i > 0 and words_and_emoticons[i - 1].lower() == \"no\") \\\n",
    "               or (i > 1 and words_and_emoticons[i - 2].lower() == \"no\") \\\n",
    "               or (i > 2 and words_and_emoticons[i - 3].lower() == \"no\" and words_and_emoticons[i - 1].lower() in [\"or\", \"nor\"] ):\n",
    "                valence = self.lexicon[item_lowercase] * N_SCALAR\n",
    "            \n",
    "            # check if sentiment laden word is in ALL CAPS (while others aren't)\n",
    "            if item.isupper() and is_cap_diff:\n",
    "                if valence > 0:\n",
    "                    valence += C_INCR\n",
    "                else:\n",
    "                    valence -= C_INCR\n",
    "\n",
    "            for start_i in range(0, 3):\n",
    "                # dampen the scalar modifier of preceding words and emoticons\n",
    "                # (excluding the ones that immediately preceed the item) based\n",
    "                # on their distance from the current item.\n",
    "                if i > start_i and words_and_emoticons[i - (start_i + 1)].lower() not in self.lexicon:\n",
    "                    s = scalar_inc_dec(words_and_emoticons[i - (start_i + 1)], valence, is_cap_diff)\n",
    "                    if start_i == 1 and s != 0:\n",
    "                        s = s * 0.95\n",
    "                    if start_i == 2 and s != 0:\n",
    "                        s = s * 0.9\n",
    "                    valence = valence + s\n",
    "                    valence = self._negation_check(valence, words_and_emoticons, start_i, i)\n",
    "                    if start_i == 2:\n",
    "                        valence = self._special_idioms_check(valence, words_and_emoticons, i)\n",
    "\n",
    "            valence = self._least_check(valence, words_and_emoticons, i)\n",
    "        sentiments.append(valence)\n",
    "        return sentiments\n",
    "\n",
    "    def _least_check(self, valence, words_and_emoticons, i):\n",
    "        # check for negation case using \"least\"\n",
    "        if i > 1 and words_and_emoticons[i - 1].lower() not in self.lexicon \\\n",
    "                and words_and_emoticons[i - 1].lower() == \"least\":\n",
    "            if words_and_emoticons[i - 2].lower() != \"at\" and words_and_emoticons[i - 2].lower() != \"very\":\n",
    "                valence = valence * N_SCALAR\n",
    "        elif i > 0 and words_and_emoticons[i - 1].lower() not in self.lexicon \\\n",
    "                and words_and_emoticons[i - 1].lower() == \"least\":\n",
    "            valence = valence * N_SCALAR\n",
    "        return valence\n",
    "\n",
    "    @staticmethod\n",
    "    def _but_check(words_and_emoticons, sentiments):\n",
    "        # check for modification in sentiment due to contrastive conjunction 'but'\n",
    "        words_and_emoticons_lower = [str(w).lower() for w in words_and_emoticons]\n",
    "        if 'but' in words_and_emoticons_lower:\n",
    "            bi = words_and_emoticons_lower.index('but')\n",
    "            for sentiment in sentiments:\n",
    "                si = sentiments.index(sentiment)\n",
    "                if si < bi:\n",
    "                    sentiments.pop(si)\n",
    "                    sentiments.insert(si, sentiment * 0.5)\n",
    "                elif si > bi:\n",
    "                    sentiments.pop(si)\n",
    "                    sentiments.insert(si, sentiment * 1.5)\n",
    "        return sentiments\n",
    "\n",
    "    @staticmethod\n",
    "    def _special_idioms_check(valence, words_and_emoticons, i):\n",
    "        words_and_emoticons_lower = [str(w).lower() for w in words_and_emoticons]\n",
    "        onezero = \"{0} {1}\".format(words_and_emoticons_lower[i - 1], words_and_emoticons_lower[i])\n",
    "\n",
    "        twoonezero = \"{0} {1} {2}\".format(words_and_emoticons_lower[i - 2],\n",
    "                                          words_and_emoticons_lower[i - 1], words_and_emoticons_lower[i])\n",
    "\n",
    "        twoone = \"{0} {1}\".format(words_and_emoticons_lower[i - 2], words_and_emoticons_lower[i - 1])\n",
    "\n",
    "        threetwoone = \"{0} {1} {2}\".format(words_and_emoticons_lower[i - 3],\n",
    "                                           words_and_emoticons_lower[i - 2], words_and_emoticons_lower[i - 1])\n",
    "\n",
    "        threetwo = \"{0} {1}\".format(words_and_emoticons_lower[i - 3], words_and_emoticons_lower[i - 2])\n",
    "\n",
    "        sequences = [onezero, twoonezero, twoone, threetwoone, threetwo]\n",
    "\n",
    "        for seq in sequences:\n",
    "            if seq in SPECIAL_CASE_IDIOMS:\n",
    "                valence = SPECIAL_CASE_IDIOMS[seq]\n",
    "                break\n",
    "\n",
    "        if len(words_and_emoticons_lower) - 1 > i:\n",
    "            zeroone = \"{0} {1}\".format(words_and_emoticons_lower[i], words_and_emoticons_lower[i + 1])\n",
    "            if zeroone in SPECIAL_CASE_IDIOMS:\n",
    "                valence = SPECIAL_CASE_IDIOMS[zeroone]\n",
    "        if len(words_and_emoticons_lower) - 1 > i + 1:\n",
    "            zeroonetwo = \"{0} {1} {2}\".format(words_and_emoticons_lower[i], words_and_emoticons_lower[i + 1],\n",
    "                                              words_and_emoticons_lower[i + 2])\n",
    "            if zeroonetwo in SPECIAL_CASE_IDIOMS:\n",
    "                valence = SPECIAL_CASE_IDIOMS[zeroonetwo]\n",
    "\n",
    "        # check for booster/dampener bi-grams such as 'sort of' or 'kind of'\n",
    "        n_grams = [threetwoone, threetwo, twoone]\n",
    "        for n_gram in n_grams:\n",
    "            if n_gram in BOOSTER_DICT:\n",
    "                valence = valence + BOOSTER_DICT[n_gram]\n",
    "        return valence\n",
    "\n",
    "    @staticmethod\n",
    "    def _sentiment_laden_idioms_check(valence, senti_text_lower):\n",
    "        # Future Work\n",
    "        # check for sentiment laden idioms that don't contain a lexicon word\n",
    "        idioms_valences = []\n",
    "        for idiom in SENTIMENT_LADEN_IDIOMS:\n",
    "            if idiom in senti_text_lower:\n",
    "                print(idiom, senti_text_lower)\n",
    "                valence = SENTIMENT_LADEN_IDIOMS[idiom]\n",
    "                idioms_valences.append(valence)\n",
    "        if len(idioms_valences) > 0:\n",
    "            valence = sum(idioms_valences) / float(len(idioms_valences))\n",
    "        return valence\n",
    "\n",
    "    @staticmethod\n",
    "    def _negation_check(valence, words_and_emoticons, start_i, i):\n",
    "        words_and_emoticons_lower = [str(w).lower() for w in words_and_emoticons]\n",
    "        if start_i == 0:\n",
    "            if negated([words_and_emoticons_lower[i - (start_i + 1)]]):  # 1 word preceding lexicon word (w/o stopwords)\n",
    "                valence = valence * N_SCALAR\n",
    "        if start_i == 1:\n",
    "            if words_and_emoticons_lower[i - 2] == \"never\" and \\\n",
    "                    (words_and_emoticons_lower[i - 1] == \"so\" or\n",
    "                     words_and_emoticons_lower[i - 1] == \"this\"):\n",
    "                valence = valence * 1.25\n",
    "            elif words_and_emoticons_lower[i - 2] == \"without\" and \\\n",
    "                    words_and_emoticons_lower[i - 1] == \"doubt\":\n",
    "                valence = valence\n",
    "            elif negated([words_and_emoticons_lower[i - (start_i + 1)]]):  # 2 words preceding the lexicon word position\n",
    "                valence = valence * N_SCALAR\n",
    "        if start_i == 2:\n",
    "            if words_and_emoticons_lower[i - 3] == \"never\" and \\\n",
    "                    (words_and_emoticons_lower[i - 2] == \"so\" or words_and_emoticons_lower[i - 2] == \"this\") or \\\n",
    "                    (words_and_emoticons_lower[i - 1] == \"so\" or words_and_emoticons_lower[i - 1] == \"this\"):\n",
    "                valence = valence * 1.25\n",
    "            elif words_and_emoticons_lower[i - 3] == \"without\" and \\\n",
    "                    (words_and_emoticons_lower[i - 2] == \"doubt\" or words_and_emoticons_lower[i - 1] == \"doubt\"):\n",
    "                valence = valence\n",
    "            elif negated([words_and_emoticons_lower[i - (start_i + 1)]]):  # 3 words preceding the lexicon word position\n",
    "                valence = valence * N_SCALAR\n",
    "        return valence\n",
    "\n",
    "    def _punctuation_emphasis(self, text):\n",
    "        # add emphasis from exclamation points and question marks\n",
    "        ep_amplifier = self._amplify_ep(text)\n",
    "        qm_amplifier = self._amplify_qm(text)\n",
    "        punct_emph_amplifier = ep_amplifier + qm_amplifier\n",
    "        return punct_emph_amplifier\n",
    "\n",
    "    @staticmethod\n",
    "    def _amplify_ep(text):\n",
    "        # check for added emphasis resulting from exclamation points (up to 4 of them)\n",
    "        ep_count = text.count(\"!\")\n",
    "        if ep_count > 4:\n",
    "            ep_count = 4\n",
    "        # (empirically derived mean sentiment intensity rating increase for\n",
    "        # exclamation points)\n",
    "        ep_amplifier = ep_count * 0.292\n",
    "        return ep_amplifier\n",
    "\n",
    "    @staticmethod\n",
    "    def _amplify_qm(text):\n",
    "        # check for added emphasis resulting from question marks (2 or 3+)\n",
    "        qm_count = text.count(\"?\")\n",
    "        qm_amplifier = 0\n",
    "        if qm_count > 1:\n",
    "            if qm_count <= 3:\n",
    "                # (empirically derived mean sentiment intensity rating increase for\n",
    "                # question marks)\n",
    "                qm_amplifier = qm_count * 0.18\n",
    "            else:\n",
    "                qm_amplifier = 0.96\n",
    "        return qm_amplifier\n",
    "\n",
    "    @staticmethod\n",
    "    def _sift_sentiment_scores(sentiments):\n",
    "        # want separate positive versus negative sentiment scores\n",
    "        pos_sum = 0.0\n",
    "        neg_sum = 0.0\n",
    "        neu_count = 0\n",
    "        for sentiment_score in sentiments:\n",
    "            if sentiment_score > 0:\n",
    "                pos_sum += (float(sentiment_score) + 1)  # compensates for neutral words that are counted as 1\n",
    "            if sentiment_score < 0:\n",
    "                neg_sum += (float(sentiment_score) - 1)  # when used with math.fabs(), compensates for neutrals\n",
    "            if sentiment_score == 0:\n",
    "                neu_count += 1\n",
    "        return pos_sum, neg_sum, neu_count\n",
    "\n",
    "    def score_valence(self, sentiments, text):\n",
    "        if sentiments:\n",
    "            sum_s = float(sum(sentiments))\n",
    "            # compute and add emphasis from punctuation in text\n",
    "            punct_emph_amplifier = self._punctuation_emphasis(text)\n",
    "            if sum_s > 0:\n",
    "                sum_s += punct_emph_amplifier\n",
    "            elif sum_s < 0:\n",
    "                sum_s -= punct_emph_amplifier\n",
    "\n",
    "            compound = normalize(sum_s)\n",
    "            # discriminate between positive, negative and neutral sentiment scores\n",
    "            pos_sum, neg_sum, neu_count = self._sift_sentiment_scores(sentiments)\n",
    "\n",
    "            if pos_sum > math.fabs(neg_sum):\n",
    "                pos_sum += punct_emph_amplifier\n",
    "            elif pos_sum < math.fabs(neg_sum):\n",
    "                neg_sum -= punct_emph_amplifier\n",
    "\n",
    "            total = pos_sum + math.fabs(neg_sum) + neu_count\n",
    "            pos = math.fabs(pos_sum / total)\n",
    "            neg = math.fabs(neg_sum / total)\n",
    "            neu = math.fabs(neu_count / total)\n",
    "\n",
    "        else:\n",
    "            compound = 0.0\n",
    "            pos = 0.0\n",
    "            neg = 0.0\n",
    "            neu = 0.0\n",
    "\n",
    "        sentiment_dict = \\\n",
    "            {\"neg\": round(neg, 3),\n",
    "             \"neu\": round(neu, 3),\n",
    "             \"pos\": round(pos, 3),\n",
    "             \"compound\": round(compound, 4)}\n",
    "\n",
    "        return sentiment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER is smart, handsome, and funny.----------------------------- {'neg': 0.0, 'neu': 0.254, 'pos': 0.746, 'compound': 0.8316}\n",
      "VADER is smart, handsome, and funny!----------------------------- {'neg': 0.0, 'neu': 0.248, 'pos': 0.752, 'compound': 0.8439}\n",
      "VADER is very smart, handsome, and funny.------------------------ {'neg': 0.0, 'neu': 0.299, 'pos': 0.701, 'compound': 0.8545}\n",
      "VADER is VERY SMART, handsome, and FUNNY.------------------------ {'neg': 0.0, 'neu': 0.246, 'pos': 0.754, 'compound': 0.9227}\n",
      "VADER is VERY SMART, handsome, and FUNNY!!!---------------------- {'neg': 0.0, 'neu': 0.233, 'pos': 0.767, 'compound': 0.9342}\n",
      "VADER is VERY SMART, uber handsome, and FRIGGIN FUNNY!!!--------- {'neg': 0.0, 'neu': 0.294, 'pos': 0.706, 'compound': 0.9469}\n",
      "VADER is not smart, handsome, nor funny.------------------------- {'neg': 0.646, 'neu': 0.354, 'pos': 0.0, 'compound': -0.7424}\n",
      "The book was good.----------------------------------------------- {'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}\n",
      "At least it isn't a horrible book.------------------------------- {'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.431}\n",
      "The book was only kind of good.---------------------------------- {'neg': 0.0, 'neu': 0.697, 'pos': 0.303, 'compound': 0.3832}\n",
      "The plot was good, but the characters are uncompelling and the dialog is not great. {'neg': 0.327, 'neu': 0.579, 'pos': 0.094, 'compound': -0.7042}\n",
      "Today SUX!------------------------------------------------------- {'neg': 0.779, 'neu': 0.221, 'pos': 0.0, 'compound': -0.5461}\n",
      "Today only kinda sux! But I'll get by, lol----------------------- {'neg': 0.127, 'neu': 0.556, 'pos': 0.317, 'compound': 0.5249}\n",
      "Make sure you :) or :D today!------------------------------------ {'neg': 0.0, 'neu': 0.294, 'pos': 0.706, 'compound': 0.8633}\n",
      "Catch utf-8 emoji such as ðŸ’˜ and ðŸ’‹ and ðŸ˜-------------------------- {'neg': 0.0, 'neu': 0.721, 'pos': 0.279, 'compound': 0.7003}\n",
      "Not bad at all--------------------------------------------------- {'neg': 0.0, 'neu': 0.513, 'pos': 0.487, 'compound': 0.431}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # --- examples -------\n",
    "    sentences = [\"VADER is smart, handsome, and funny.\",  # positive sentence example\n",
    "                 \"VADER is smart, handsome, and funny!\",\n",
    "                 # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
    "                 \"VADER is very smart, handsome, and funny.\",\n",
    "                 # booster words handled correctly (sentiment intensity adjusted)\n",
    "                 \"VADER is VERY SMART, handsome, and FUNNY.\",  # emphasis for ALLCAPS handled\n",
    "                 \"VADER is VERY SMART, handsome, and FUNNY!!!\",\n",
    "                 # combination of signals - VADER appropriately adjusts intensity\n",
    "                 \"VADER is VERY SMART, uber handsome, and FRIGGIN FUNNY!!!\",\n",
    "                 # booster words & punctuation make this close to ceiling for score\n",
    "                 \"VADER is not smart, handsome, nor funny.\",  # negation sentence example\n",
    "                 \"The book was good.\",  # positive sentence\n",
    "                 \"At least it isn't a horrible book.\",  # negated negative sentence with contraction\n",
    "                 \"The book was only kind of good.\",\n",
    "                 # qualified positive sentence is handled correctly (intensity adjusted)\n",
    "                 \"The plot was good, but the characters are uncompelling and the dialog is not great.\",\n",
    "                 # mixed negation sentence\n",
    "                 \"Today SUX!\",  # negative slang with capitalization emphasis\n",
    "                 \"Today only kinda sux! But I'll get by, lol\",\n",
    "                 # mixed sentiment example with slang and constrastive conjunction \"but\"\n",
    "                 \"Make sure you :) or :D today!\",  # emoticons handled\n",
    "                 \"Catch utf-8 emoji such as ðŸ’˜ and ðŸ’‹ and ðŸ˜\",  # emojis handled\n",
    "                 \"Not bad at all\"  # Capitalized negation\n",
    "                 ]\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    for sentence in sentences:\n",
    "        vs = analyzer.polarity_scores(sentence)\n",
    "        print(\"{:-<65} {}\".format(sentence, str(vs)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
